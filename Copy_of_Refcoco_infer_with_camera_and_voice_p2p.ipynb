{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nermienkh/BMD212_Spring22/blob/main/Copy_of_Refcoco_infer_with_camera_and_voice_p2p.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VqwUvUcJ7uQ"
      },
      "source": [
        "## **OFA**\n",
        "Start to enjoy visual grounding with OFA! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo9qN06Ym6Jg",
        "outputId": "746fe623-e876-412a-9b8c-36abd6cff457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'OFA'...\n",
            "remote: Enumerating objects: 6610, done.\u001b[K\n",
            "remote: Counting objects: 100% (226/226), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 6610 (delta 113), reused 171 (delta 86), pack-reused 6384\u001b[K\n",
            "Receiving objects: 100% (6610/6610), 122.86 MiB | 32.27 MiB/s, done.\n",
            "Resolving deltas: 100% (2721/2721), done.\n"
          ]
        }
      ],
      "source": [
        "# clone OFA if there does not exist the repo\n",
        "%cd /content\n",
        "!git clone https://github.com/OFA-Sys/OFA.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVHcnR_yKJsF"
      },
      "source": [
        "## **Download Checkpoint**\n",
        "We provide a link for our public checkpoint, and you only need to wget it to your workspace. We also provide an alternative below. Choose one as you like!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPScYGm3cv4b",
        "outputId": "a9991317-9fab-45e3-9688-bb4459e3e1df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mDFLq5xC01M"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')\n",
        "!mkdir -p /content/OFA/checkpoints\n",
        "!ln -s /content/Drive/MyDrive/caption.pt /content/OFA/checkpoints/refcocog.pt\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbN_v4QwKSUO",
        "outputId": "318e4dcc-f710-4ca5-864b-b659a612c178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!mkdir -p /content/OFA/checkpoints/\n",
        "#uncomment this part if file is not downloaded and dont run the next cell which point to Drive \n",
        "#!wget https://ofa-silicon.oss-us-west-1.aliyuncs.com/checkpoints/refcocog_large_best.pt\n",
        "#!mv refcocog_large_best.pt OFA/checkpoints/refcocog.pt\n",
        "#!cp /content/drive/MyDrive/RefCoco-Visualgrounding/refcocog_large_best.pt OFA/checkpoints/refcocog.pt\n",
        "#ln Make links and symlinks between files or directories.\n",
        "!cp /content/drive/MyDrive/RefCoco-Visualgrounding/refcocog_large_best.pt /content/OFA/checkpoints/refcocog.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrz9BgHBL0ew"
      },
      "source": [
        "## **Install Fairseq**\n",
        "We advise you to install fairseq by cloning the official repository and running \"pip install\". \n",
        "\n",
        "You should restart the window if you meet the hint of \"RESTART RUNTIME\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GPlAa3iL7iR",
        "outputId": "e627ba1e-3d5f-4127-defb-a87bb8bcecf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 34534, done.\u001b[K\n",
            "remote: Total 34534 (delta 0), reused 0 (delta 0), pack-reused 34534\u001b[K\n",
            "Receiving objects: 100% (34534/34534), 24.06 MiB | 28.91 MiB/s, done.\n",
            "Resolving deltas: 100% (25109/25109), done.\n",
            "Note: switching to '6795311bfeb9d39fe11a62803184b81acb66509e'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "/content/fairseq\n",
            "\u001b[33mDEPRECATION: In-tree builds are now the default. pip 22.1 will enforce this behaviour change. A possible replacement is to remove the --use-feature=in-tree-build flag.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.0) (1.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.0) (4.65.0)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.0) (1.22.4)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.6/269.6 KB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.0) (2022.10.31)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.0) (0.29.33)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.0) (0.13.1+cu116)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from omegaconf<2.1->fairseq==0.12.0) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.9/dist-packages (from omegaconf<2.1->fairseq==0.12.0) (6.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.0) (4.9.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.0) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi->fairseq==0.12.0) (2.21)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.0-cp39-cp39-linux_x86_64.whl size=19061261 sha256=2b84b18a4b7cfb91078c8e58d7f56883977d23769186ac8970502b8ed36884d9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4hv1nc0o/wheels/52/da/57/31b8a8f767e4d044de3fbb1f204d0f1547e8c6e0b171e56bba\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=4758032b2fd21b5a4d994dd695b2e33e558f080221df9e80ac75884945296ce2\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/3c/ae/14db087e6018de74810afe32eb6ac890ef9c68ba19b00db97a\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.3 colorama-0.4.6 fairseq-0.12.0 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1\n"
          ]
        }
      ],
      "source": [
        "# clone Fairseq for installation\n",
        "%cd /content\n",
        "!git clone https://github.com/pytorch/fairseq.git -b v0.12.0\n",
        "%cd /content/fairseq\n",
        "\n",
        "!pip install --use-feature=in-tree-build ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwdxKi3NL_m6"
      },
      "source": [
        "## **Preparation**\n",
        "Below you just need to import required packages, and check whether to use GPU or FP16. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s0sPKqsMAOi",
        "outputId": "72e39bcc-970c-4b17-d091-a438745c764e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/OFA\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from opencv-python) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.9/dist-packages (0.6.12)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from timm) (0.13.3)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.9/dist-packages (from timm) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm) (0.14.1+cu116)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (3.10.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy==6.0.3 in /usr/local/lib/python3.9/dist-packages (6.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from ftfy==6.0.3) (0.2.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX==2.4.1 in /usr/local/lib/python3.9/dist-packages (2.4.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX==2.4.1) (3.19.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardX==2.4.1) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools==2.0.4 in /usr/local/lib/python3.9/dist-packages (2.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pycocotools==2.0.4) (1.22.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from pycocotools==2.0.4) (3.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (4.39.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (1.0.7)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (5.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (3.0.9)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools==2.0.4) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0.4) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocoevalcap==1.2 in /usr/local/lib/python3.9/dist-packages (1.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from pycocoevalcap==1.2) (2.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.22.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.7.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.39.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.9/dist-packages (2.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (0.8.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (1.13.1+cu116)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (2023.3.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (0.11.4)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (1.22.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.27.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.9/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.9/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.9/dist-packages (0.5.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.9/dist-packages (0.10.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.3.4)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.22.4)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.7.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa) (63.4.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa) (23.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa) (3.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.9/dist-packages (0.20.9)\n",
            "Requirement already satisfied: Levenshtein==0.20.9 in /usr/local/lib/python3.9/dist-packages (from python-Levenshtein) (0.20.9)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.9/dist-packages (from Levenshtein==0.20.9->python-Levenshtein) (2.13.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: zhconv in /usr/local/lib/python3.9/dist-packages (1.4.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pypinyin==0.47.1 in /usr/local/lib/python3.9/dist-packages (0.47.1)\n"
          ]
        }
      ],
      "source": [
        "%cd /content/OFA\n",
        "!sed '1d' requirements.txt | xargs -I {} pip install {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1APBsXk2MC26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d968cc-b2ac-4efb-d397-fff8a6f6e32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: g2p-en in /usr/local/lib/python3.9/dist-packages (2.1.0)\n",
            "Requirement already satisfied: distance>=0.1.3 in /usr/local/lib/python3.9/dist-packages (from g2p-en) (0.1.3)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.9/dist-packages (from g2p-en) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.9/dist-packages (from g2p-en) (1.22.4)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from g2p-en) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from inflect>=0.3.1->g2p-en) (1.10.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.4->g2p-en) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.4->g2p-en) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.4->g2p-en) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.4->g2p-en) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=1.9.1->inflect>=0.3.1->g2p-en) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "!pip install g2p-en #added\n",
        "from fairseq import utils, tasks\n",
        "from fairseq import checkpoint_utils\n",
        "from utils.eval_utils import eval_step\n",
        "from tasks.mm_tasks.refcoco import RefcocoTask\n",
        "from models.ofa import OFAModel\n",
        "from PIL import Image\n",
        "\n",
        "# Register refcoco task\n",
        "tasks.register_task('refcoco', RefcocoTask)\n",
        "\n",
        "# turn on cuda if GPU is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# use fp16 only when GPU is available\n",
        "use_fp16 = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8zara7cNf8v"
      },
      "source": [
        "## **Build Model**\n",
        "Below you can build your model and load the weights from the given checkpoint, and also build a generator. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nYzxjbEzNew5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77076d2d-f546-4820-9e4d-9df99363ebeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained ckpt & config\n",
        "overrides={\"bpe_dir\":\"utils/BPE\"}\n",
        "models, cfg, task = checkpoint_utils.load_model_ensemble_and_task(\n",
        "        utils.split_paths('checkpoints/refcocog.pt'),\n",
        "        arg_overrides=overrides\n",
        "    )\n",
        "\n",
        "cfg.common.seed = 7\n",
        "cfg.generation.beam = 5\n",
        "cfg.generation.min_len = 4\n",
        "cfg.generation.max_len_a = 0\n",
        "cfg.generation.max_len_b = 4\n",
        "cfg.generation.no_repeat_ngram_size = 3\n",
        "\n",
        "# Fix seed for stochastic decoding\n",
        "if cfg.common.seed is not None and not cfg.generation.no_seed_provided:\n",
        "    np.random.seed(cfg.common.seed)\n",
        "    utils.set_torch_seed(cfg.common.seed)\n",
        "    \n",
        "\n",
        "# Move models to GPU\n",
        "for model in models:\n",
        "    model.eval()\n",
        "    if use_fp16:\n",
        "        model.half()\n",
        "    if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n",
        "        model.cuda()\n",
        "    model.prepare_for_inference_(cfg)\n",
        "\n",
        "# Initialize generator\n",
        "generator = task.build_generator(models, cfg.generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pot0EzT1SaPm"
      },
      "source": [
        "## **Preprocess**\n",
        "We demonstrate the required transformation fucntions for preprocessing inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Yq9akfBKSa3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc22f69-c6d7-4e0a-99b4-ccb112abbb23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Image transform\n",
        "from torchvision import transforms\n",
        "mean = [0.5, 0.5, 0.5]\n",
        "std = [0.5, 0.5, 0.5]\n",
        "\n",
        "patch_resize_transform = transforms.Compose([\n",
        "    lambda image: image.convert(\"RGB\"),\n",
        "    transforms.Resize((cfg.task.patch_image_size, cfg.task.patch_image_size),interpolation=Image.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# Text preprocess\n",
        "bos_item = torch.LongTensor([task.src_dict.bos()])\n",
        "eos_item = torch.LongTensor([task.src_dict.eos()])\n",
        "pad_idx = task.src_dict.pad()\n",
        "def encode_text(text, length=None, append_bos=False, append_eos=False):\n",
        "    s = task.tgt_dict.encode_line(\n",
        "        line=task.bpe.encode(text.lower()),\n",
        "        add_if_not_exist=False,\n",
        "        append_eos=False\n",
        "    ).long()\n",
        "    if length is not None:\n",
        "        s = s[:length]\n",
        "    if append_bos:\n",
        "        s = torch.cat([bos_item, s])\n",
        "    if append_eos:\n",
        "        s = torch.cat([s, eos_item])\n",
        "    return s\n",
        "\n",
        "# Construct input for refcoco task\n",
        "patch_image_size = cfg.task.patch_image_size\n",
        "def construct_sample(image: Image, text: str):\n",
        "    \n",
        "    w,h = image.size\n",
        "\n",
        "    w_resize_ratio = torch.tensor(patch_image_size / w).unsqueeze(0)\n",
        "    h_resize_ratio = torch.tensor(patch_image_size / h).unsqueeze(0)\n",
        "    patch_image = patch_resize_transform(image).unsqueeze(0)\n",
        "    patch_mask = torch.tensor([True])\n",
        "    src_text = encode_text(' which region does the text \" {} \" describe?'.format(text), append_bos=True, append_eos=True).unsqueeze(0)\n",
        "    src_length = torch.LongTensor([s.ne(pad_idx).long().sum() for s in src_text])\n",
        "    sample = {\n",
        "        \"id\":np.array(['42']),\n",
        "        \"image\":image # added\n",
        "        ,\n",
        "\n",
        "        \"net_input\": {\n",
        "            \"src_tokens\": src_text,\n",
        "            \"src_lengths\": src_length,\n",
        "            \"patch_images\": patch_image,\n",
        "            \"patch_masks\": patch_mask,\n",
        "        },\n",
        "        \"w_resize_ratios\": w_resize_ratio,\n",
        "        \"h_resize_ratios\": h_resize_ratio,\n",
        "        \"region_coords\": torch.randn(1, 4)\n",
        "    }\n",
        "    return sample\n",
        "  \n",
        "# Function to turn FP32 to FP16\n",
        "def apply_half(t):\n",
        "    if t.dtype is torch.float32:\n",
        "        return t.to(dtype=torch.half)\n",
        "    return t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZHHeZ8XCCRK"
      },
      "source": [
        "## **Run Inference**\n",
        "Download an image and run the following scripts to generate the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-mvEomkCCqT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "9b4ef031-5931-4113-e49d-87700aa31f63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Download an image from COCO or you can use other images with wget\\n#! wget https://ofa-silicon.oss-us-west-1.aliyuncs.com/datasets/refcoco/pokemon.jpg\\n#! mv pokemon.jpg  test.jpg\\n#image = Image.open(\\'./test.jpg\\')\\n#image = Image.open(\\'/content/collection.jpg\\')\\n\\n#text = \"an orange turtle-like pokemon with round head\"\\ntext = \"where is the laptop?\"\\n\\n# Construct input sample & preprocess for GPU if cuda available\\n#sample = construct_sample(image, text)\\n#sample = utils.move_to_cuda(sample) if use_cuda else sample\\n#sample = utils.apply_to_sample(apply_half, sample) if use_fp16 else sample\\n\\n# Run eval step for refcoco\\nwith torch.no_grad():\\n    result, scores = eval_step(task, generator, models, sample)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "# Download an image from COCO or you can use other images with wget\n",
        "#! wget https://ofa-silicon.oss-us-west-1.aliyuncs.com/datasets/refcoco/pokemon.jpg\n",
        "#! mv pokemon.jpg  test.jpg\n",
        "#image = Image.open('./test.jpg')\n",
        "#image = Image.open('/content/collection.jpg')\n",
        "\n",
        "#text = \"an orange turtle-like pokemon with round head\"\n",
        "text = \"where is the laptop?\"\n",
        "\n",
        "# Construct input sample & preprocess for GPU if cuda available\n",
        "#sample = construct_sample(image, text)\n",
        "#sample = utils.move_to_cuda(sample) if use_cuda else sample\n",
        "#sample = utils.apply_to_sample(apply_half, sample) if use_fp16 else sample\n",
        "\n",
        "# Run eval step for refcoco\n",
        "with torch.no_grad():\n",
        "    result, scores = eval_step(task, generator, models, sample)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "B1bRRE1lG_99",
        "outputId": "56986520-42cc-4a6b-9f09-ef3766daf838"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport cv2\\nimport numpy\\nfrom google.colab.patches import cv2_imshow\\n\\nimg = cv2.cvtColor(numpy.asarray(image), cv2.COLOR_RGB2BGR)\\nrect=(int(result[0][\"box\"][0]), int(result[0][\"box\"][1]),int(result[0][\"box\"][2]), int(result[0][\"box\"][3]))\\n\\n#\\tcv.rectangle(\\timg, pt1, pt2, color[, thickness[, lineType[, shift]]]\\t) -> \\timg\\ncv2.rectangle(\\n    img,\\n    (rect[0], rect[1]),\\n    (rect[2], rect[3]),\\n    (0, 255, 0),\\n    3\\n)\\ncv2_imshow(img)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "import cv2\n",
        "import numpy\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "img = cv2.cvtColor(numpy.asarray(image), cv2.COLOR_RGB2BGR)\n",
        "rect=(int(result[0][\"box\"][0]), int(result[0][\"box\"][1]),int(result[0][\"box\"][2]), int(result[0][\"box\"][3]))\n",
        "\n",
        "#\tcv.rectangle(\timg, pt1, pt2, color[, thickness[, lineType[, shift]]]\t) -> \timg\n",
        "cv2.rectangle(\n",
        "    img,\n",
        "    (rect[0], rect[1]),\n",
        "    (rect[2], rect[3]),\n",
        "    (0, 255, 0),\n",
        "    3\n",
        ")\n",
        "cv2_imshow(img)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MyxpZwF-fxGD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model helper function to run detection on image\n",
        "def model_helper(img,info,  width=1, height=1):\n",
        "  # Construct input sample & preprocess for GPU if cuda available\n",
        "  sample = construct_sample(img, info)\n",
        "  sample = utils.move_to_cuda(sample) if use_cuda else sample\n",
        "  sample = utils.apply_to_sample(apply_half, sample) if use_fp16 else sample\n",
        "\n",
        "  # get image ratios to convert bounding boxes to proper size\n",
        "  img_height, img_width = img.size\n",
        "  width_ratio = img_width/width\n",
        "  height_ratio = img_height/height\n",
        "\n",
        "  # run model on darknet style image to get detections\n",
        "  # Run eval step for refcoco\n",
        "  with torch.no_grad():\n",
        "    result, scores = eval_step(task, generator, models, sample)\n",
        "  return result, width_ratio, height_ratio,scores\n",
        "\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  #I have removed the mode to be detected on run time as i changed the overlay array size from 4 to 1 channel so we don't have an RGBA image anymore\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array )\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"Status:\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '' +\n",
        "          'When finished, click here or on the video to stop this demo';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Speech input"
      ],
      "metadata": {
        "id": "oEwRzOZ9ert-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Fq2xeWMFBR_R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "26d7cd93-59d2-4411-caf3-1a30cc9103cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from SpeechRecognition) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->SpeechRecognition) (2022.12.7)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.10.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#for speech\n",
        "!pip install ffmpeg-python\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "!pip install SpeechRecognition\n",
        "import speech_recognition as sr\n",
        "sr.__version__\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#JavaScript to properly create our Speech using our Microphone as input\n",
        "\n",
        "\"\"\"\n",
        "To write this piece of code I took inspiration/code from a lot of places.\n",
        "It was late night, so I'm not sure how much I created or just copied o.O\n",
        "Here are some of the possible references:\n",
        "https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/\n",
        "https://stackoverflow.com/a/18650249\n",
        "https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/\n",
        "https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/\n",
        "https://stackoverflow.com/a/49019356\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "  \n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr"
      ],
      "metadata": {
        "id": "9EdveUUggBMu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#record Speech\n",
        "audio, srr = get_audio()   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "id": "BvpOJcdPexIo",
        "outputId": "b924837a-8257-42bf-df17-6bc1dc54bc27"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "var my_div = document.createElement(\"DIV\");\n",
              "var my_p = document.createElement(\"P\");\n",
              "var my_btn = document.createElement(\"BUTTON\");\n",
              "var t = document.createTextNode(\"Press to start recording\");\n",
              "\n",
              "my_btn.appendChild(t);\n",
              "//my_p.appendChild(my_btn);\n",
              "my_div.appendChild(my_btn);\n",
              "document.body.appendChild(my_div);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "var recordButton = my_btn;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "  gumStream = stream;\n",
              "  var options = {\n",
              "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
              "    mimeType : 'audio/webm;codecs=opus'\n",
              "    //mimeType : 'audio/webm;codecs=pcm'\n",
              "  };            \n",
              "  //recorder = new MediaRecorder(stream, options);\n",
              "  recorder = new MediaRecorder(stream);\n",
              "  recorder.ondataavailable = function(e) {            \n",
              "    var url = URL.createObjectURL(e.data);\n",
              "    var preview = document.createElement('audio');\n",
              "    preview.controls = true;\n",
              "    preview.src = url;\n",
              "    document.body.appendChild(preview);\n",
              "\n",
              "    reader = new FileReader();\n",
              "    reader.readAsDataURL(e.data); \n",
              "    reader.onloadend = function() {\n",
              "      base64data = reader.result;\n",
              "      //console.log(\"Inside FileReader:\" + base64data);\n",
              "    }\n",
              "  };\n",
              "  recorder.start();\n",
              "  };\n",
              "\n",
              "recordButton.innerText = \"Recording... press to stop\";\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "\n",
              "function toggleRecording() {\n",
              "  if (recorder && recorder.state == \"recording\") {\n",
              "      recorder.stop();\n",
              "      gumStream.getAudioTracks()[0].stop();\n",
              "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
              "  }\n",
              "}\n",
              "\n",
              "// https://stackoverflow.com/a/951057\n",
              "function sleep(ms) {\n",
              "  return new Promise(resolve => setTimeout(resolve, ms));\n",
              "}\n",
              "\n",
              "var data = new Promise(resolve=>{\n",
              "//recordButton.addEventListener(\"click\", toggleRecording);\n",
              "recordButton.onclick = ()=>{\n",
              "toggleRecording()\n",
              "\n",
              "sleep(2000).then(() => {\n",
              "  // wait 2000ms for the data to be available...\n",
              "  // ideally this should use something like await...\n",
              "  //console.log(\"Inside data:\" + base64data)\n",
              "  resolve(base64data.toString())\n",
              "\n",
              "});\n",
              "\n",
              "}\n",
              "});\n",
              "      \n",
              "</script>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Speech to text\n",
        "\n",
        "#Google Web Speech API,\n",
        "#Caution: The default key provided by SpeechRecognition is for testing purposes only,\n",
        "# and Google may revoke it at any time. It is not a good idea to use the Google Web Speech API in production.\n",
        "# Even with a valid API key, you’ll be limited to only 50 requests per day, and there is no way to raise this quota. Fortunately, \n",
        "#SpeechRecognition’s interface is nearly identical for each API,\n",
        "# so what you learn today will be easy to translate to a real-world project.\n",
        "\n",
        "#supported languages\n",
        "#https://cloud.google.com/speech-to-text/docs/speech-to-text-supported-languages\n",
        "\n",
        "r = sr.Recognizer()\n",
        "myaudio=sr.AudioData(audio,srr,4) #2 in chrome 4 in firefox\n",
        "text= r.recognize_google(myaudio, show_all=False)    \n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV6wgARNfCtJ",
        "outputId": "34da6a42-08ce-4fc0-f477-5c6eae271411"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where is my notebook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Working Loop "
      ],
      "metadata": {
        "id": "OjsDE7dtelv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pulse2percept"
      ],
      "metadata": {
        "id": "P4aDSkPwVCfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pulse2percept as p2p\n",
        "from pulse2percept.stimuli import ImageStimulus\n",
        "#Scoreboard model of [Beyeler2019] (standalone model) Implements the scoreboard model described in [Beyeler2019], where all percepts are Gaussian blobs.\n",
        "modelp2p = p2p.models.ScoreboardModel(xrange=(-7, 7), yrange=(-7, 7), xystep=0.1)\n",
        "modelp2p.build()\n",
        "\n",
        "from pulse2percept.implants import AlphaAMS\n",
        "implant = AlphaAMS()"
      ],
      "metadata": {
        "id": "OKd_PkcX9kSl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XgAjgdQzoVBS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "outputId": "698a3d23-4377-4782-bda4-5da1cc5b127f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"Status:\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '' +\n",
              "          'When finished, click here or on the video to stop this demo';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-cc9fb9804f99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# call our model helper on video frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# loop through detections and draw them on transparent overlay image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-44643d0522f2>\u001b[0m in \u001b[0;36mmodel_helper\u001b[0;34m(img, info, width, height)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# Run eval step for refcoco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OFA/utils/eval_utils.py\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(task, generator, models, sample, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_vqa_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'refcoco'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_refcoco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'snli_ve'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_snli_ve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OFA/utils/eval_utils.py\u001b[0m in \u001b[0;36meval_refcoco\u001b[0;34m(task, generator, models, sample, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mious\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minteracts_w\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minteracts_h\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mgen_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0mhyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fairseq/tasks/fairseq_task.py\u001b[0m in \u001b[0;36minference_step\u001b[0;34m(self, generator, models, sample, prefix_tokens, constraints)\u001b[0m\n\u001b[1;32m    535\u001b[0m     ):\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             return generator.generate(\n\u001b[0m\u001b[1;32m    538\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OFA/models/sequence_generator.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, models, sample, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     def _generate(\n",
            "\u001b[0;32m/content/OFA/models/sequence_generator.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, models, sample, prefix_tokens, constraints, bos_token)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# compute the encoder output for each beam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EnsembleModel: forward_encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mencoder_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# placeholder of indices for bsz * beam_size to hold tokens and accumulative scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OFA/models/sequence_generator.py\u001b[0m in \u001b[0;36mforward_encoder\u001b[0;34m(self, net_input)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_torchscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OFA/models/sequence_generator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_torchscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fairseq/models/fairseq_encoder.py\u001b[0m in \u001b[0;36mforward_torchscript\u001b[0;34m(self, net_input)\u001b[0m\n\u001b[1;32m     53\u001b[0m             )\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_non_torchscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munused\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fairseq/models/fairseq_encoder.py\u001b[0m in \u001b[0;36mforward_non_torchscript\u001b[0;34m(self, net_input)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"prev_output_tokens\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         }\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreorder_encoder_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OFA/models/ofa/unify_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_tokens, src_lengths, patch_images, patch_images_2, patch_masks, code_masks, return_all_hiddens, token_embeddings, sample_patch_num)\u001b[0m\n\u001b[1;32m    795\u001b[0m                   \u001b[0mOnly\u001b[0m \u001b[0mpopulated\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \"\"\"\n\u001b[0;32m--> 797\u001b[0;31m         return self.forward_scriptable(src_tokens,\n\u001b[0m\u001b[1;32m    798\u001b[0m                                        \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                                        \u001b[0mpatch_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OFA/models/ofa/unify_transformer.py\u001b[0m in \u001b[0;36mforward_scriptable\u001b[0;34m(self, src_tokens, src_lengths, patch_images, patch_images_2, patch_masks, return_all_hiddens, token_embeddings, sample_patch_num)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0mprompt_kv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m             x = layer(x, encoder_padding_mask=encoder_padding_mask if has_pads else None, \\\n\u001b[0m\u001b[1;32m    936\u001b[0m                     self_attn_bias=self_attn_bias, prompt_kv=prompt_kv)\n\u001b[1;32m    937\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "    # i added this line here as the Image variable is getting overrided  each iteration and  idk why yet?!\n",
        "    from PIL import Image\n",
        "    # convert JS response to OpenCV Image\n",
        "    Opencv_frame = js_to_image(js_reply[\"img\"])\n",
        "    #convert image to gray scale\n",
        "    gray = cv2.cvtColor(Opencv_frame, cv2.COLOR_BGR2GRAY)\n",
        "     # convert  OpenCV Image to PIL\n",
        "    frame = Image.fromarray(gray)\n",
        "    #generate initial mask \n",
        "    #mask = np.zeros(Opencv_frame.shape, dtype=\"uint8\")\n",
        "\n",
        "    # For reversing the operation:\n",
        "    #im_np = np.asarray(im_pil)\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([frame.size[1],frame.size[0],1], dtype=np.uint8)\n",
        "  \n",
        "\n",
        "    # call our model helper on video frame\n",
        "    result, width_ratio, height_ratio,score = model_helper(frame,text)\n",
        "\n",
        "    # loop through detections and draw them on transparent overlay image\n",
        "    rect=(int(result[0][\"box\"][0]), int(result[0][\"box\"][1]),int(result[0][\"box\"][2]), int(result[0][\"box\"][3]))\n",
        "  \n",
        "    #left, top, right, bottom = int(rect[0] * width_ratio), int(rect[1] * height_ratio), int(rect[2] * width_ratio), int(rect[3] * height_ratio)\n",
        "    left, top, right, bottom = int(rect[0] ), int(rect[1] ), int(rect[2] ), int(rect[3] )\n",
        "    #trnasform mask based on the detected object\n",
        "    #mask=cv2.rectangle(mask, (left, top), (right, bottom), 255, -1)\n",
        "   \n",
        "    bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), (255, 255, 255),-1)\n",
        "    bbox_array = cv2.bitwise_and(np.asarray(frame),np.asarray( frame), mask=bbox_array)\n",
        "    \n",
        "    im = Image.fromarray(bbox_array)\n",
        "    im.save(\"object.png\")\n",
        "\n",
        "    #pulse2percept part\n",
        "    \n",
        "    stim = ImageStimulus(\"object.png\")\n",
        "    stim_edge = stim.filter('canny')\n",
        "    implant.stim = stim_edge.resize(implant.shape)\n",
        "    #Then the implant can be passed to the model’s predict_percept method:\n",
        "    percept_gray = modelp2p.predict_percept(implant)\n",
        "     \n",
        "    np_img = np.squeeze(percept_gray.data, axis=2)  # axis=2 is channel dimension \n",
        "   \n",
        "\n",
        "    # convert overlay of bbox into bytes\n",
        "    #bbox_bytes = bbox_to_bytes((percept_gray.data *255).astype(np.uint8))\n",
        "    bbox_bytes = bbox_to_bytes((np_img*255).astype(np.uint8))\n",
        "\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes\n",
        "\n",
        "    \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.squeeze(percept_gray.data,axis=2).shape"
      ],
      "metadata": {
        "id": "jj_oU8knsb8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d7078d-a84f-4e0c-bcf9-cfa5d6089969"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(141, 141)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(np_img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3uvE091XAXA",
        "outputId": "1b66d78d-54ed-4ed5-c142-70f69586f67e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pil_img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "JSV9tE8nf-0l",
        "outputId": "5de85098-eda2-4856-e840-bc4588c33b44"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_OUTMODES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'F'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PNG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2241\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot write mode {mode} as PNG\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: cannot write mode F as PNG",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PNG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not save to PNG for display\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not save to PNG for display"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=F size=141x141 at 0x7FC82E479880>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stim.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "nrE4PUp2VQV4",
        "outputId": "5e375172-daee-4440-c028-1161c385a9d7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2NBk9u2VePG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}